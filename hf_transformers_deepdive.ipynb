{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using HF Transfomers\n",
    "\n",
    "Deepdive into the HF transformer `pipeline()` function. \n",
    "\n",
    "It‚Äôs important to understand how it works under the hood. This notebook covers...\n",
    "\n",
    "- how to use tokenizers and models to **replicate the pipeline()** function‚Äôs behavior\n",
    "- how to **load and save models and tokenizers**\n",
    "- Different **tokenization approaches**\n",
    "- how to handle multiple sentences of varying lengths\n",
    "\n",
    "Context.\n",
    "\n",
    "Transformer models are usually very large. With millions to tens of billions of parameters, training and deploying these models is a complicated undertaking. Furthermore, with new models being released on a near-daily basis and each having its own implementation, trying them all out is no easy task.\n",
    "\n",
    "The HF Transformers library was created to solve this problem. Its goal is to **provide a single API** through which any Transformer model can be loaded, trained, and saved. \n",
    "The library‚Äôs main features are:\n",
    "\n",
    "- **Ease of use:** Downloading, loading, and using a state-of-the-art NLP model for inference can be done in just two lines of code.\n",
    "- **Flexibility:** At their core, all models are simple PyTorch `nn.Module` or TensorFlow `tf.keras.Model` classes and can be handled like any other models in their respective machine learning (ML) frameworks.\n",
    "- **Simplicity:** Hardly any abstractions are made across the library. The **‚ÄúAll in one file‚Äù is a core concept**: a model‚Äôs forward pass is **entirely defined in a single file**, so that the code itself is understandable and hackable.\n",
    "\n",
    "This last feature makes ü§ó Transformers quite different from other ML libraries. The models are not built on modules that are shared across files; instead, each model has its own layers. In addition to making the models more approachable and understandable, this allows you to easily experiment on one model without affecting others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## End-to-end example \n",
    "\n",
    "Starting with a model and a tokenizer together to replicate the pipeline() function\n",
    "\n",
    "Component 1: Model API\n",
    "Component 2: Tokenizer API\n",
    "\n",
    "Tokenizers take care of the first and last processing steps, handling the conversion from text to numerical inputs for the neural network, and the conversion back to text when it is needed. \n",
    "\n",
    "**What happends inside the Pipeline() function?**\n",
    "\n",
    "<img src=\"https://huggingface.co/course/static/chapter2/full_nlp_pipeline.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9987136125564575},\n",
       " {'label': 'POSITIVE', 'score': 0.9998186230659485}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### What was the pipeline?\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "raw_inputs = [\n",
    "    \"I wanted to got to othe Tacos place last Friday but there's was a queue as long as I would expect in front of the Berghain. So I skipped it.\",\n",
    "    \"The dinner was pretty tasty ;-)\"\n",
    "    ]\n",
    "classifier(raw_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing with a tokenizer\n",
    "\n",
    "Like other NN, Transformer models can‚Äôt process raw text directly, so the first step of our pipeline is to **convert the text inputs into numbers** that the model can make sense of. \n",
    "\n",
    "- **Splitting** the input into words, subwords, or symbols (like punctuation) that are called tokens\n",
    "- **Mapping** each token to an integer\n",
    "- Adding additional inputs that may be useful to the model\n",
    "\n",
    "All this **preprocessing needs to be done in exactly the same way as when the model was pretrained**, so we first need to download that information from the Model Hub. To do this, we use the `AutoTokenizer` class and its `from_pretrained()` method. Using the checkpoint name of our model, it will automatically fetch the data associated with the model‚Äôs tokenizer and cache it (so it‚Äôs only downloaded the first time you run the code below).\n",
    "\n",
    "Since the default checkpoint of the sentiment-analysis pipeline is distilbert-base-uncased-finetuned-sst-2-english, we run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='distilbert-base-uncased-finetuned-sst-2-english', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)  # Get the tokenizer specific to this model\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Once we have the tokenizer, we can directly pass our sentences to it and we‚Äôll get back a dictionary that‚Äôs ready to feed to our model! \n",
    "- The only thing left to do is to **convert the list of input IDs to tensors**.\n",
    "\n",
    "Using Transformers we do not need to worry about which ML framework is used as a backend; it might be PyTorch or TensorFlow, or Flax for some models. \n",
    "\n",
    "However, **Transformer models only accept tensors** as input. \n",
    "\n",
    "To specify the type of tensors we want to get back (PyTorch, TensorFlow, or plain NumPy), we use the return_tensors argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(2, 40), dtype=int32, numpy=\n",
      "array([[  101,  1045,  2359,  2000,  2288,  2000, 27178,  5369, 11937,\n",
      "        13186,  2173,  2197,  5958,  2021,  2045,  1005,  1055,  2001,\n",
      "         1037, 24240,  2004,  2146,  2004,  1045,  2052,  5987,  1999,\n",
      "         2392,  1997,  1996, 15214, 10932,  2078,  1012,  2061,  1045,\n",
      "        16791,  2009,  1012,   102],\n",
      "       [  101,  1996,  4596,  2001,  3492, 11937, 21756,  1025,  1011,\n",
      "         1007,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 40), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "      dtype=int32)>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-16 11:51:50.031150: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-16 11:51:50.050311: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fc12dd68050 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-11-16 11:51:50.050323: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors='tf')\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output itself is a dictionary containing two keys, **input_ids** and **attention_mask**. \n",
    "\n",
    "- input_ids contains two rows of integers (one for each sentence) that are the **unique identifiers** **of the tokens** in each sentence. \n",
    "- We‚Äôll explain what the attention_mask is later in this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going through the model\n",
    "\n",
    "We can download our pretrained model the same way we did with our tokenizer. HF Transformers provides an `TFAutoModel` class which also has a `from_pretrained` method:\n",
    "\n",
    "- This architecture contains only the **base Transformer module**: given some inputs, it outputs what we‚Äôll call hidden states, also known as features. \n",
    "- For each model input, we‚Äôll retrieve a **high-dimensional vector** representing the contextual understanding of that input by the Transformer model.\n",
    "\n",
    "More details: \n",
    "- While these **hidden states** can be useful on their own, they‚Äôre **usually inputs to another part of the model**, known as the **head**. In Chapter 1, the \n",
    "- different tasks can be performed with the same architecture, but **each of these tasks will have a different head** associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-16 11:51:50.943673: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertModel: ['pre_classifier', 'dropout_19', 'classifier']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModel\n",
    "\n",
    "checkpoint = checkpoint  # i.e. same model as above\n",
    "model = TFAutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **vector output** by the Transformer module is usually large. It generally has **three dimensions**:\n",
    "\n",
    "- **Batch size**: The number of sequences processed at a time (2 in this example).\n",
    "- **Sequence length**: The length of the numerical representation of the sequence (40 in this example).\n",
    "- **Hidden size**: The vector dimension of each model input.\n",
    "\n",
    "It is said to be ‚Äúhigh dimensional‚Äù because of the last value. The hidden size can be very large (768 is common for smaller models, and in larger models this can reach 3072 or more)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 2 \n",
      " Sequence Length: 40 \n",
      " Hidden Size (Vector Dimension): 768\n"
     ]
    }
   ],
   "source": [
    "outputs = model(inputs)  # Feeding tokenized input into model \n",
    "\n",
    "print(\n",
    "    \"Batch Size:\", outputs.last_hidden_state.shape[0], \"\\n\",\n",
    "    \"Sequence Length:\", outputs.last_hidden_state.shape[1], \"\\n\",\n",
    "    \"Hidden Size (Vector Dimension):\", outputs.last_hidden_state.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the outputs of HF Transformers models behave like namedtuples or dictionaries. \n",
    "- we can access the elements by \n",
    "  - attributes (`outputs.last_hidden_state.shape[1]`)\n",
    "  - by key (`outputs[\"last_hidden_state\"]`), \n",
    "  - or by index if you know exactly where the thing you are looking for is (`outputs[0]`).\n",
    "\n",
    "#### Model heads: Making sense out of numbers\n",
    "\n",
    "- The model heads take the **high-dimensional vector of hidden states as input and project them onto a different dimension**. \n",
    "- They are usually composed of one or a few linear layers:\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/course/static/chapter2/transformer_and_head.png\" width=\"50%\">\n",
    "\n",
    "The output of the Transformer model is sent directly to the model head to be processed.\n",
    "\n",
    "In the diagram above the model is represented by ...\n",
    "\n",
    "- its embeddings layer and the subsequent layers (transformer network). \n",
    "  - the embeddings layer **converts** each input ID in the tokenized input into a **vector** that represents the associated token. \n",
    "  - the subsequent layers manipulate those vectors using the **attention mechanism** to produce the **final representation** of the sentences.\n",
    "\n",
    "There are many different architectures available in HF transformers library, with each one designed around tackling a specific task. Some examples: \n",
    "\n",
    "*Model (retrieve the hidden states)\n",
    "*ForCausalLM\n",
    "*ForMaskedLM\n",
    "*ForMultipleChoice\n",
    "*ForQuestionAnswering\n",
    "*ForSequenceClassification\n",
    "*ForTokenClassification\n",
    "\n",
    " and more....\n",
    "\n",
    "Sentiment Example:\n",
    "\n",
    "In our running example we need a model with a **sequence classification head** (to be able to classify the sentences as positive or negative). So, we won‚Äôt actually use the TFAutoModel class, but TFAutoModelForSequenceClassification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remember our model was:  distilbert-base-uncased-finetuned-sst-2-english\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_58']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "print(\"Remember our model was: \", checkpoint)  # get the model name\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)  # get the model weights\n",
    "outputs = model(inputs)  # input the tokenized raw_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we look at the shape of our inputs, the dimensionality will be **much lower**: the model head takes as input the high-dimensional vectors we saw before, and outputs vectors containing **two values** (one per label):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs shape (Batch Size, Labels) (2, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Outputs shape (Batch Size, Labels)\", outputs.logits.shape)\n",
    "                                                                                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocessing the output\n",
    "\n",
    "The values we get as output from our model don‚Äôt necessarily make sense by themselves. Let‚Äôs take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 3.6506321 -3.0039456]\n",
      " [-4.1607122  4.454075 ]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our model predicted **[ 3.6506321 -3.0039456]** for the first sentence and **[-4.1607122  4.454075 ]** for the second one. \n",
    "- Those are **not probabilities but logits**, the raw, unnormalized scores outputted by the last layer of the model. \n",
    "- To be **converted to probabilities**, they need to go through a **SoftMax** layer\n",
    "\n",
    "all HF  Transformers models output the logits, as the loss function for training will generally fuse the last activation function, such as SoftMax, with the actual loss function, such as cross entropy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[9.9871361e-01 1.2864548e-03]\n",
      " [1.8137053e-04 9.9981862e-01]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "predictions = tf.math.softmax(outputs.logits, axis = -1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the **model predicted** \n",
    "\n",
    "1. [9.9871361e-01 1.2864548e-03] for the first sentence and \n",
    "2. [1.8137053e-04 9.9981862e-01] for the second one. \n",
    "3. \n",
    "4. These are recognizable probability scores.\n",
    "\n",
    "To get the labels corresponding to each position, we can inspect the **id2label** attribute of the model config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'NEGATIVE', 1: 'POSITIVE'} \n",
      " Remember results from the full pipeline classifier:  [{'label': 'NEGATIVE', 'score': 0.9987136125564575}, {'label': 'POSITIVE', 'score': 0.9998186230659485}]\n"
     ]
    }
   ],
   "source": [
    "print(model.config.id2label, \"\\n\", \"Remember results from the full pipeline classifier: \", classifier(raw_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline results match the replicated results so we successfully reproduced the pipeline. \n",
    "\n",
    "**Preprocess with Tokenizers -> Passing the inputs through the model and postprocessing**\n",
    "\n",
    "Full code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select Model distilbert-base-uncased-finetuned-sst-2-english\n",
      "######################### \n",
      " Step 1: Tokenization\n",
      "Raw Inputs:  [\"I wanted to got to othe Tacos place last Friday but there's was a queue as long as I would expect in front of the Berghain. So I skipped it.\", 'The dinner was pretty tasty ;-)']\n",
      "Tokenized Inputs:  {'input_ids': <tf.Tensor: shape=(2, 40), dtype=int32, numpy=\n",
      "array([[  101,  1045,  2359,  2000,  2288,  2000, 27178,  5369, 11937,\n",
      "        13186,  2173,  2197,  5958,  2021,  2045,  1005,  1055,  2001,\n",
      "         1037, 24240,  2004,  2146,  2004,  1045,  2052,  5987,  1999,\n",
      "         2392,  1997,  1996, 15214, 10932,  2078,  1012,  2061,  1045,\n",
      "        16791,  2009,  1012,   102],\n",
      "       [  101,  1996,  4596,  2001,  3492, 11937, 21756,  1025,  1011,\n",
      "         1007,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 40), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "      dtype=int32)>}\n",
      "######################### \n",
      " Step 2: Modeling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertModel: ['pre_classifier', 'dropout_19', 'classifier']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden States: \n",
      " Batch Size: 2 \n",
      " Sequence Length: 40 \n",
      " Hidden Size (Vector Dimension): 768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_253']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs shape \n",
      " (Batch Size, Labels) (2, 2)\n",
      "######################### \n",
      " Step 3: Postprecessing\n",
      "Logits:  tf.Tensor(\n",
      "[[ 3.6506321 -3.0039456]\n",
      " [-4.1607122  4.454075 ]], shape=(2, 2), dtype=float32)\n",
      "Prediction:  tf.Tensor(\n",
      "[[9.9871361e-01 1.2864548e-03]\n",
      " [1.8137053e-04 9.9981862e-01]], shape=(2, 2), dtype=float32)\n",
      "{0: 'NEGATIVE', 1: 'POSITIVE'}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModel\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "# Select Model\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "print(\"Select Model\", checkpoint)\n",
    "\n",
    "print(\"######################### \\n\", \"Step 1: Tokenization\")\n",
    "print(\"Raw Inputs: \", raw_inputs)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)  # Get the tokenizer specific to this model\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors='tf')\n",
    "print(\"Tokenized Inputs: \", inputs)\n",
    "\n",
    "print(\"######################### \\n\", \"Step 2: Modeling\")\n",
    "# Either use full model\n",
    "model = TFAutoModel.from_pretrained(checkpoint)\n",
    "outputs = model(inputs)  # Feeding tokenized input into model \n",
    "\n",
    "print(\n",
    "    \"Hidden States: \\n\",\n",
    "    \"Batch Size:\", outputs.last_hidden_state.shape[0], \"\\n\",\n",
    "    \"Sequence Length:\", outputs.last_hidden_state.shape[1], \"\\n\",\n",
    "    \"Hidden Size (Vector Dimension):\", outputs.last_hidden_state.shape[2])\n",
    "\n",
    "# Or use exact model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)  # get the model weights\n",
    "outputs = model(inputs)  # input the tokenized raw_input\n",
    "\n",
    "print(\"Outputs shape \\n (Batch Size, Labels)\", outputs.logits.shape)\n",
    "\n",
    "print(\"######################### \\n\", \"Step 3: Postprecessing\")\n",
    "print(\"Logits: \", outputs.logits)\n",
    "\n",
    "predictions = tf.math.softmax(outputs.logits, axis = -1)\n",
    "print(\"Prediction: \", predictions)\n",
    "print(model.config.id2label)\n",
    "                                                                                   "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
