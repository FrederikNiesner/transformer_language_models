{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Transformer model \"from scratch\"\n",
    "\n",
    "This notebook covers: \n",
    "\n",
    "- steps for creating and using a model use the HF TFAutoModel \n",
    "  - `TFAutoModel` class and all of its relatives are actually simple wrappers over the wide variety of models available in the library. \n",
    "  - It can automatically \"guess\" the appropriate model architecture for your checkpoint, and then instantiates a model with this architecture.\n",
    "  - However, if you already know the type of model you want to use, you can use the `class` that defines its architecture directly. \n",
    "- Using BERT model as an example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Initialize a BERT model with configuration object.\n",
      "Building the config:...\n",
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Building the model (from config):...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-16 13:23:36.837423: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-16 13:23:36.865010: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7feeffb7bbc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-11-16 13:23:36.865021: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 1: Initialize a BERT model with configuration object.\")\n",
    "\n",
    "from transformers import TFBertModel, BertConfig\n",
    "\n",
    "print(\"Building the config:...\")\n",
    "config = BertConfig()\n",
    "print(config)\n",
    "\n",
    "print(\"Building the model (from config):...\")\n",
    "model = TFBertModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The model can be used in this state, but it will output gibberish because if we load a model **this way it needs to be trained first**. \n",
    "- We could train the model from scratch on the task at hand, but this would require a long time and a lot of data, and it would have a non-negligible environmental impact. \n",
    "\n",
    "- To avoid unnecessary and duplicated effort, it’s imperative to be able to **share and reuse models that have already been trained**. We can do this using the `from_pretrained()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 570/570 [00:00<00:00, 207kB/s]\n",
      "Downloading: 100%|██████████| 502M/502M [00:17<00:00, 29.9MB/s]\n",
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We could replace `TFBertModel` with the equivalent `TFAutoModel` class. This produces **checkpoint-agnostic code**, i.e. if your code works for one checkpoint, it should work seamlessly with another. \n",
    "- This applies even if the architecture is different, as long as the checkpoint was trained for a **similar task** (for example, a sentiment analysis task).\n",
    "- In the code sample above we didn’t use BertConfig, and instead loaded a pretrained model via the bert-base-cased identifier. \n",
    "- This is a **model checkpoint** that was trained by the authors of BERT themselves; you can find more details about it in its [model card](https://huggingface.co/bert-base-cased).\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "This model is now initialized with all the weights of the checkpoint. It \n",
    "\n",
    "- **can now be used directly for inference on the tasks it was trained on** OR \n",
    "- **fine-tuned on a new task**. By training with pretrained weights rather than from scratch, we can quickly achieve good results.\n",
    "\n",
    "The weights have been downloaded and cached (so future calls to the `from_pretrained()` method won’t re-download them) in the **cache folder**  (default: `~/.cache/huggingface/transformers`) . You can customize your cache folder by setting the `HF_HOME` environment variable.\n",
    "\n",
    "The identifier used to load the model can be the identifier of any model on the Model Hub, as long as it is **compatible with the BERT architecture**. The entire list of available BERT checkpoints can be found [here](https://huggingface.co/models?filter=bert).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving methods\n",
    "\n",
    "Saving a model is as easy as loading one - we use the `save_pretrained()` method, which is analogous to the `from_pretrained()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "myMacDirectory = \"models/bertBaseCased\"\n",
    "model.save_pretrained(myMacDirectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will save a **config.json** file with the **attributes** necessary to build the model **architecture**. This file also contains some **metadata**, such as where the checkpoint originated and what HF Transformers version you were using when you last saved the checkpoint.\n",
    "\n",
    "The **tf_model.h5** file is known as the **state dictionary**; it contains all your **model’s weights**. \n",
    "\n",
    "The two files go hand in hand; the configuration is necessary to know your **model’s architecture**, while the model weights are your **model’s parameters**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelConfig = open(\"models/bertBaseCased/config.json\")\n",
    "print(modelConfig.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Transformer model for inference (predictions)\n",
    "\n",
    "**We did load and save** a BERT model. Next: **Let’s try using it to make some predictions**. \n",
    "\n",
    "- Model Inputs need to be numeric. \n",
    "- Tokenizers can take care of casting the inputs to the appropriate framework’s tensors\n",
    "- The tokenizer converts these to vocabulary indices which are typically called **input IDs**. \n",
    "- Each sequence is now a list of numbers! \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 101 7592  999  102]\n",
      " [ 101 4658 1012  102]\n",
      " [ 101 3835  999  102]], shape=(3, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Example: Let’s say we have a couple of sequences:\n",
    "sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]\n",
    "\n",
    "encoded_sequences = [\n",
    "    [101, 7592, 999, 102],\n",
    "    [101, 4658, 1012, 102],\n",
    "    [101, 3835, 999, 102],\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "# Convert to tensor\n",
    "import tensorflow as tf\n",
    "model_inputs = tf.constant(encoded_sequences)\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tensor can now be used for the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-16 13:58:27.958642: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_93']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "output = model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  tf.Tensor(\n",
      "[[4.0195312e-02 9.5980465e-01]\n",
      " [5.3534308e-04 9.9946469e-01]], shape=(2, 2), dtype=float32)\n",
      "{0: 'NEGATIVE', 1: 'POSITIVE'}\n"
     ]
    }
   ],
   "source": [
    "predictions = tf.math.softmax(output.logits, axis = -1)\n",
    "print(\"Prediction: \", predictions)\n",
    "print(model.config.id2label)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
