{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with huggingface transformers in tensorflow.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with pipelines\n",
    "The most basic object in the ü§ó Transformers library is the pipeline. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 255M/255M [00:12<00:00, 20.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Pipeline covers preprocessing->model->post-processing\n",
    "classifier = pipeline(\"sentiment-analysis\")  # Download and cache classifier object. Default is english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.990067720413208},\n",
       " {'label': 'NEGATIVE', 'score': 0.99828040599823},\n",
       " {'label': 'NEGATIVE', 'score': 0.9944549202919006},\n",
       " {'label': 'NEGATIVE', 'score': 0.9997929930686951}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass multiple sentences to the classifier.\n",
    "classifier(\n",
    "    [\"I love Hamburg. But it's always rainy.\",\n",
    "    \"The movie was great and the actors were bad.\",\n",
    "    \"No man ever steps in the same river twice, for it's not the same river not the same\",\n",
    "    \"I do not at all agree with the results of this useless classifier.\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What other pipelines do we have?\n",
    "\n",
    "- feature-extraction (get the vector representation of a text)\n",
    "- fill-mask\n",
    "- ner (named entity recognition)\n",
    "- question-answering\n",
    "- sentiment-analysis\n",
    "- summarization\n",
    "- text-generation\n",
    "- translation\n",
    "- zero-shot-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot classification\n",
    "\n",
    "We‚Äôll start by tackling a more challenging task where we need to classify texts that haven‚Äôt been labelled. This is a common scenario in real-world projects because annotating text is usually time-consuming and requires domain expertise. For this use case, the zero-shot-classification pipeline is very powerful: it allows you to specify which labels to use for the classification, so you don‚Äôt have to rely on the labels of the pretrained model. You‚Äôve already seen how the model can classify a sentence as positive or negative using those two labels ‚Äî but it can also classify the text using any other set of labels you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli (https://huggingface.co/facebook/bart-large-mnli)\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.13k/1.13k [00:00<00:00, 435kB/s]\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.52G/1.52G [01:41<00:00, 16.0MB/s]\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26.0/26.0 [00:00<00:00, 12.7kB/s]\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 878k/878k [00:00<00:00, 1.14MB/s]\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 446k/446k [00:00<00:00, 1.02MB/s]\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.29M/1.29M [00:01<00:00, 1.22MB/s]\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'I love you so much like I never loved you before',\n",
       "  'labels': ['love', 'fun', 'discussion', 'marriage'],\n",
       "  'scores': [0.8779177069664001,\n",
       "   0.06078661233186722,\n",
       "   0.039918605238199234,\n",
       "   0.021377060562372208]},\n",
       " {'sequence': 'But I think you could do the dishes and vacuum clean the flat more often, please',\n",
       "  'labels': ['discussion', 'fun', 'marriage', 'love'],\n",
       "  'scores': [0.8770378232002258,\n",
       "   0.10134901851415634,\n",
       "   0.011419342830777168,\n",
       "   0.01019375491887331]},\n",
       " {'sequence': 'You ruined my live.',\n",
       "  'labels': ['discussion', 'marriage', 'fun', 'love'],\n",
       "  'scores': [0.7202327251434326,\n",
       "   0.13064803183078766,\n",
       "   0.11257608979940414,\n",
       "   0.03654312714934349]},\n",
       " {'sequence': 'Shut up and go on the toilette.',\n",
       "  'labels': ['discussion', 'fun', 'marriage', 'love'],\n",
       "  'scores': [0.5608827471733093,\n",
       "   0.29711979627609253,\n",
       "   0.11796832829713821,\n",
       "   0.024029165506362915]},\n",
       " {'sequence': 'Tell us a joke because your jokes are always so funny.',\n",
       "  'labels': ['fun', 'discussion', 'marriage', 'love'],\n",
       "  'scores': [0.7312267422676086,\n",
       "   0.24712669849395752,\n",
       "   0.010939897038042545,\n",
       "   0.010706735774874687]},\n",
       " {'sequence': 'Denim fashio jeans are the hottest shit right now on the market.',\n",
       "  'labels': ['fun', 'love', 'discussion', 'marriage'],\n",
       "  'scores': [0.5056102275848389,\n",
       "   0.27309754490852356,\n",
       "   0.21197892725467682,\n",
       "   0.009313303977251053]}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\n",
    "    [\"I love you so much like I never loved you before\",\n",
    "    \"But I think you could do the dishes and vacuum clean the flat more often, please\",\n",
    "    \"You ruined my live.\",\n",
    "    \"Shut up and go on the toilette.\",\n",
    "    \"Tell us a joke because your jokes are always so funny.\",\n",
    "    \"Denim fashio jeans are the hottest shit right now on the market.\"],\n",
    "    candidate_labels=[\"love\", \"fun\", \"discussion\", \"marriage\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': \"Toggling in the report is highly distracting and sometimes I inadvertently am looking at a whole year vs just a quarter.  Filters vary between reports which can lead to results that are close, but not fully-aligned.  We don't include forecast to end-of-quarter... it would be great to be able to clearly show consultants how many hrs/wk they need to hit to meet their target (based on where they are at now).  I find that I have to 'reset' the report more often than I'd like and then need to go back an re-select filters.\",\n",
       "  'labels': ['feature request', 'bug', 'kudos'],\n",
       "  'scores': [0.5338761806488037, 0.3844722807407379, 0.08165154606103897]},\n",
       " {'sequence': \"At the current moment, i do no think the utilization report is updated to include all client facing work to count under the new 'attainable client facing' metric. I have not checked in the last week but the last time I checked, some of my attainable metric was not accurate due to Free Work budgets not being counted under client facing attainable.\",\n",
       "  'labels': ['bug', 'feature request', 'kudos'],\n",
       "  'scores': [0.5300726294517517, 0.2842258810997009, 0.18570147454738617]},\n",
       " {'sequence': 'Overall, happy with the ease of use and access. Kudos to the team.',\n",
       "  'labels': ['kudos', 'feature request', 'bug'],\n",
       "  'scores': [0.9817205667495728, 0.010077547281980515, 0.008201917633414268]},\n",
       " {'sequence': 'Getting numbers into the system earlier in the year would be great...always feel like we are playing catchup throughout Q1.',\n",
       "  'labels': ['feature request', 'bug', 'kudos'],\n",
       "  'scores': [0.4817098379135132, 0.27978768944740295, 0.23850250244140625]},\n",
       " {'sequence': \"I need to see the consultants forecast on a rolling 13 week view, not by quarter. Eg. from today's date or this week, what is the forecast for the next 13 weeks? \",\n",
       "  'labels': ['feature request', 'bug', 'kudos'],\n",
       "  'scores': [0.8804190754890442, 0.07446260750293732, 0.045118298381567]}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will return the probability for each label in sorted array (decreasing probabilty)\n",
    "\n",
    "classifier(\n",
    "    [\"Toggling in the report is highly distracting and sometimes I inadvertently am looking at a whole year vs just a quarter.  Filters vary between reports which can lead to results that are close, but not fully-aligned.  We don't include forecast to end-of-quarter... it would be great to be able to clearly show consultants how many hrs/wk they need to hit to meet their target (based on where they are at now).  I find that I have to 'reset' the report more often than I'd like and then need to go back an re-select filters.\",\n",
    "    \"At the current moment, i do no think the utilization report is updated to include all client facing work to count under the new 'attainable client facing' metric. I have not checked in the last week but the last time I checked, some of my attainable metric was not accurate due to Free Work budgets not being counted under client facing attainable.\",\n",
    "    \"Overall, happy with the ease of use and access. Kudos to the team.\",\n",
    "    \"Getting numbers into the system earlier in the year would be great...always feel like we are playing catchup throughout Q1.\",\n",
    "    \"I need to see the consultants forecast on a rolling 13 week view, not by quarter. Eg. from today's date or this week, what is the forecast for the next 13 weeks? \"],\n",
    "    candidate_labels=[\"bug\", \"feature request\", \"kudos\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generation\n",
    "\n",
    "Now let‚Äôs see how to use a pipeline to generate some text. The main idea here is that you provide a prompt and the model will auto-complete it by generating the remaining text. This is similar to the predictive text feature that is found on many phones. Text generation involves randomness, so it‚Äôs normal if you don‚Äôt get the same results as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 (https://huggingface.co/gpt2)\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 523M/523M [00:29<00:00, 18.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'As a Data Scientist at Adobe I mostly work with SQL Server, and use my own tools, Excel Analyzer and Bounded File system (BNR). These tools are also used for the development of this code, which uses a very sophisticated technique for'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"As a Data Scientist at Adobe I mostly work with\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"As a Data Engineer at Adobe I'd like to offer you my best understanding of Adobe's Data Analytics Platform. It is simple, easy, and completely free.\\n\\nWhat's more, it is completely flexible. Adobe has designed this platform so that you can add, change, or delete data without any delay and it has two very powerful features which include:\\n\\nData is stored on your Windows hard drive\\n\\nYou do not have to think big about where and where to add data or modify\"},\n",
       " {'generated_text': \"As a Data Engineer at Adobe I've been in this position for more than 6 years now. I was fortunate enough to have the opportunity to work through the last year of my career and learn the core competencies of an entire company while covering most of the most exciting and difficult topics in the data science field. I can't think of a more qualified position to be a part of!\\n\\nWhat has changed since you took this gig?\\n\\nI've been working in C++ for about 8\"},\n",
       " {'generated_text': 'As a Data Engineer at Adobe I was working on a program for Excel that showed how to use the data you have created. I used this on a recent dataset and it was completely awesome!\\n\\nAt Adobe I also applied to an R class doing data analysis for business clients. I found that it felt right to see how data can be used during a business session and how quickly you can perform analysis. I was told that what would happen if I worked on a database full of data, was that'},\n",
       " {'generated_text': \"As a Data Engineer at Adobe I love what I do. I spent about 3 years as an expert engineer in Data Science at Adobe, an institution of higher education that is the world's largest data science school ‚Äî I spent 3 years as a Data Scientist at Adobe, an institution of higher education that is the world's largest data science school.\\n\\nSo, I have studied how organizations perform across various technologies, including data analytics, data storage infrastructure, data analytics, and business intelligence. I have tried\"},\n",
       " {'generated_text': \"As a Data Engineer at Adobe I also understand that we need to keep up with new developments and it's only worth it to take a look at a demo of something that I'm sure we'll soon be taking the time to implement in future apps as well.\\n\\nI've got a bit more technical background in data science and I'm a Data Scientist, so I'm a bit less familiar with Data Science as we know it today.\\n\\nIf you would like to learn about Data Science,\"}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"As a Data Engineer at Adobe I\", num_return_sequences=5, max_length=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a specific model from the Huggingface Hub in a pipeline: GPT-2\n",
    "\n",
    "Go to the Model Hub and click on the corresponding tag on the left to display only the supported models for that task. You should get to a page like this one.\n",
    "\n",
    "Let‚Äôs try the distilgpt2 model! Here‚Äôs how to load it in the same pipeline as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline, set_seed\n",
    "# generator = pipeline('text-generation', model='distilgpt2')\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "# set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, and a programming environment. I'm not a data scientist and I'm not a programmer. But all I do\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not an interpreter.\\n\\nIt's not that I'm only using language, or that I want to rewrite\"},\n",
       " {'generated_text': 'Hello, I\\'m a language model, that\\'s what it sounds like.\" \"My name is Mika.\" \"Is there any way you can make'},\n",
       " {'generated_text': \"Hello, I'm a language model, not a theory. (You will get a nice little tutorial on my blog if you are interested in a bit\"},\n",
       " {'generated_text': 'Hello, I\\'m a language model, and it\\'s a good thing. I think the problem I solve is, \"Why are so many languages different'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations and bias\n",
    "\n",
    "The training data used for this model has not been released as a dataset one can browse. We know it contains a lot of unfiltered content from the internet, which is far from neutral. As the openAI team themselves point out in their model card:\n",
    "\n",
    "_Because large-scale language models like GPT-2 do not distinguish fact from fiction, we don‚Äôt support use-cases that require the generated text to be true._\n",
    "\n",
    "_Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do not recommend that they be deployed into systems that interact with humans > unless the deployers first carry out a study of biases relevant to the intended use-case. We found no statistically significant difference in gender, race, and religious bias probes between 774M and 1.5B, implying all versions of GPT-2 should be approached with similar levels of caution around use cases that are sensitive to biases around human attributes._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The White man worked as a clerk at the old'},\n",
       " {'generated_text': 'The White man worked as a salesman in Mexico and'},\n",
       " {'generated_text': 'The White man worked as a lawyer in the White'},\n",
       " {'generated_text': 'The White man worked as a clerk for the store'},\n",
       " {'generated_text': 'The White man worked as a barkeep and was'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"The White man worked as a\", max_length=10, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The Black man worked as a clerk at the old'},\n",
       " {'generated_text': 'The Black man worked as a salesman in Mexico and'},\n",
       " {'generated_text': 'The Black man worked as a lawyer in the city'},\n",
       " {'generated_text': 'The Black man worked as a clerk for the store'},\n",
       " {'generated_text': 'The Black man worked as a barkeep and was'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "generator(\"The Black man worked as a\", max_length=10, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 487M/487M [00:33<00:00, 15.4MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der Sinn des Lebens ist es, eine gute Nachricht f√ºr die Gesellschaft zu senden.\n",
      "In der Welt gibt es einige Leute, die behaupten, dass \"Gott zu einem der gr√∂√üten S√ºnder geworden\" sei, und sich von solchen Dingen fernhalten m√∂chten.\n",
      "So kann man das verstehen.\n",
      "Was aber ist dies?\n",
      "Ich sage dir, Gott hat sich selbst f√ºr einen der gr√∂√üten S√ºnder geworden.\n",
      "In den 70er Jahren kamen viele Leute auf diese Meinung, als ihr Verstand f√ºr das Gute in der Welt\n"
     ]
    }
   ],
   "source": [
    "# Testing another language\n",
    "pipe = pipeline('text-generation', model=\"dbmdz/german-gpt2\",\n",
    "                 tokenizer=\"dbmdz/german-gpt2\")\n",
    "\n",
    "text = pipe(\"Der Sinn des Lebens ist es\", max_length=100)[0][\"generated_text\"]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base (https://huggingface.co/distilroberta-base)\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 316M/316M [00:16<00:00, 19.8MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'This course will teach you all about mathematical models.',\n",
       "  'score': 0.19619858264923096,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical'},\n",
       " {'sequence': 'This course will teach you all about computational models.',\n",
       "  'score': 0.04052719101309776,\n",
       "  'token': 38163,\n",
       "  'token_str': ' computational'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\")\n",
    "unmasker(\"This course will teach you all about <mask> models.\", top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top_k argument controls how many possibilities you want to be displayed. Note that here the model fills in the special <mask> word, which is often referred to as a mask token. Other mask-filling models might have different mask tokens, so it‚Äôs always good to verify the proper mask word when exploring other models. One way to check it is by looking at the mask word used in the widget."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation\n",
    "\n",
    "Translation model from english to spanish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This usually works but somehow requires a fresh / new notebook. Not sure yet what the issue is. \n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "translator(\"Ce cours est produit par Hugging Face.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whats inside a pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do Transformers work?\n",
    "\n",
    "### General Architecture\n",
    "\n",
    "**Encoder (left)** The encoder receives an input and builds a representation of its features. i.e. the model is optimized to acquire **understanding** from the input\n",
    "\n",
    "**Decoder (right)** The decoder uses the encoder's representation (features) along with other inputs to **generate** a target sequence. This means that the model is optimized. \n",
    "\n",
    "Example Use Cases: \n",
    "\n",
    "* Encoder Only: Good for tasts that require **understanding** of the input, such as sentence **classification** and **named entity recognition**.\n",
    "* Decoder Only: Good for **generative** tasks. \n",
    "* BOTH, i.e. encoder-decoder / sequence-to-sequence models: Good for generative tasks that recquire an input, such as **translation** or **summarization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Encoder Decoder](https://miro.medium.com/max/856/1*ZCFSvkKtppgew3cc7BIaug.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Layers\n",
    "\n",
    "* A key feature of transformers is that they are uilt with special layers called **attention layers** which tell the model to pay specific attention to certain words in the sentence (and more or less ignore the others) when dealing with the representation of the word. \n",
    "* E.g. For grammar reasons it is important to pay attention to a close word to generate the right translation (You like...) but not so much another part of the sentence (...my books.)\n",
    "* A word by itself has a meaning, but that meaning is deeply affected by the **context**, which can be any other word (or words) before or after the word being studied. \n",
    "\n",
    "\n",
    "### The original architecture\n",
    "\n",
    "* The transformer architecture was originally designed for translation. \n",
    "* During training, the encoder receives inputs (sentences) in a certain language, while the decoder receives the same sentences in the desired target language. \n",
    "* In the encoder, the attention layers can use all the words in a sentence (since the translation of a given word can be dependent on what words before and after in the sentence). \n",
    "* The decoder, however, works sequentially and can only pay attention to the words in the sentence that it has already translated, i.e. only the the words before the word that's currently being generated.\n",
    "* To speed thigns up during training (when the model has access to target sentences), the decoder is fed the whole target, but it is not allowed to use future words.\n",
    "* Note that the first attention layer ('masked multi-head attention') in a decoder block pays attention to all (past) inputs to the decoder, but the seconde attention layer (multi-head attention) uses the output of the encoder. It can thus access the whole input sentence to best predict the current word. This is very useful as different languages can have grammatical rules that put the words in different order, or some context provided later in the sentence may be helpful to determine the best translation of a given word. \n",
    "* The **attention mask** can also be used in the encoder/decoder to prevent the model from paying attention to some special words, e.g. < oov >\n",
    "\n",
    "Finally.... some definitions. \n",
    "\n",
    "* **Architecture:** This is the skeleton of the model. The definition of each layer and each operation that happens within the model.\n",
    "* **Checkpoints:** These are the weights that will be loaded in a given architecture. \n",
    "* **Model:** Umbrella term that isn't as precises and can embody both of the above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Only Example: \n",
    "\n",
    "* Encoder models use only the encoder of a Transformer model. \n",
    "* **At each stage**, the attention layers can **access all the words** in the initial sentence. \n",
    "* These models are often characterized as having ‚Äúbi-directional‚Äù attention, and are often called **auto-encoding models**.\n",
    "\n",
    "Representatives of this family of models include:\n",
    "\n",
    "ALBERT\n",
    "BERT\n",
    "DistilBERT\n",
    "ELECTRA\n",
    "RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 249M/249M [00:12<00:00, 20.5MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   [Input]:  He had also stgruggled with addiction during his time in Congress .\n",
      "[Detected]:  He had also <i>stgruggled</i> with addiction during his time in Congress .\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "   [Input]:  The review thoroughla assessed all aspects of JLENS SuR and CPG esign maturit and confidence .\n",
      "[Detected]:  The review <i>thoroughla</i> assessed all aspects of JLENS SuR and CPG <i>esign maturit</i> and confidence .\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "   [Input]:  Letterma also apologized two his staff for the satyation .\n",
      "[Detected]:  <i>Letterma</i> also apologized <i>two</i> his staff for the <i>satyation</i> .\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "   [Input]:  Vincent Jay had earlier won France 's first gold in gthe 10km biathlon sprint .\n",
      "[Detected]:  Vincent Jay had earlier won France 's first gold in <i>gthe</i> 10km biathlon sprint .\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "   [Input]:  It is left to the directors to figure out hpw to bring the stry across to tye audience .\n",
      "[Detected]:  It is left to the directors to figure out <i>hpw</i> to bring the <i>stry</i> across to <i>tye</i> audience .\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "model_name_or_path = \"m3hrdadfi/typo-detector-distilbert-en\"\n",
    "config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name_or_path, config=config)\n",
    "nlp = pipeline('token-classification', model=model, tokenizer=tokenizer, aggregation_strategy=\"average\")\n",
    "sentences = [\n",
    " \"He had also stgruggled with addiction during his time in Congress .\",\n",
    " \"The review thoroughla assessed all aspects of JLENS SuR and CPG esign maturit and confidence .\",\n",
    " \"Letterma also apologized two his staff for the satyation .\",\n",
    " \"Vincent Jay had earlier won France 's first gold in gthe 10km biathlon sprint .\",\n",
    " \"It is left to the directors to figure out hpw to bring the stry across to tye audience .\",\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    typos = [sentence[r[\"start\"]: r[\"end\"]] for r in nlp(sentence)]\n",
    "\n",
    "    detected = sentence\n",
    "    for typo in typos:\n",
    "        detected = detected.replace(typo, f'<i>{typo}</i>')\n",
    "\n",
    "    print(\"   [Input]: \", sentence)\n",
    "    print(\"[Detected]: \", detected)\n",
    "    print(\"-\" * 130)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Only Examples\n",
    "\n",
    "* Decoder models use only the decoder of a Transformer model. \n",
    "* At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. \n",
    "* These models are often called auto-regressive models.\n",
    "* The pretraining of decoder models usually revolves around predicting the next word in the sentence.\n",
    "* These models are best suited for tasks involving **causal tasks and generating sequences.**\n",
    "* Causal Langauge Modeling: Word: \"my\" -> generate next most likely word: \"name\" -> repeat operation until satisfied.\n",
    "* Key Difference to Encoder: Self-Attention Mechanism, i.e. not bi-directional but only single context (left or right sentence)\n",
    "* \n",
    "\n",
    "Representatives of this family of models include:\n",
    "\n",
    "CTRL\n",
    "GPT\n",
    "GPT-2\n",
    "Transformer XL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence-to-Sequence\n",
    "\n",
    "* Encoder-decoder models (also called sequence-to-sequence models) use both parts of the Transformer architecture. \n",
    "* At each stage, the attention layers of the encoder can access all the words in the initial sentence, whereas the attention layers of the decoder can only access the words positioned before a given word in the input.\n",
    "* The pretraining of these models can be done using the objectives of encoder or decoder models, but usually involves something a bit more complex.\n",
    "* Sequence-to-sequence models are best suited for **tasks revolving around generating new sentences depending on a given input**, such as **summarization**, **translation**, or **generative question answering**.\n",
    "* Weights between Encoder and Decoder are not necessarily shared. This allows, that the encoder does a different task to the decoder, e.g. understand the meaning of a EN sentence. Then pass the meaning to the decoder, which tanslatetes the meaning, not the original text. \n",
    "\n",
    "Representatives of this family of models include:\n",
    "\n",
    "BART\n",
    "mBART\n",
    "Marian\n",
    "T5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This notebook includes: \n",
    "\n",
    "* how to approach different NLP tasks using the high-level ü§ó **Transformers pipeline API**. \n",
    "* how Transformer models work at a high level, and talked \n",
    "* about the importance of transfer learning and fine-tuning. \n",
    "* A key aspect is that you can use the full architecture or only the encoder or decoder, depending on what kind of task you aim to solve. \n",
    "\n",
    "| Model       | Exampels    | Tasks       |\n",
    "| ----------- | ----------- | -----------\n",
    "| Encoder     | ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa       |Sentence classification, named entity recognition, extractive question answering       |\n",
    "| Decoder     | CTRL, GPT, GPT-2, Transformer XL        |Text generation       |\n",
    "| Encoder-Decoder   | BART, T5, Marian, mBART        |Summarization, translation, generative question answering\n",
    "       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
