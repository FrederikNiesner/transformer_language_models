{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with huggingface transformers in tensorflow.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with pipelines\n",
    "The most basic object in the ðŸ¤— Transformers library is the pipeline. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_39']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Pipeline covers preprocessing->model->post-processing\n",
    "classifier = pipeline(\"sentiment-analysis\")  # Download and cache classifier object. Default is english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.990067720413208},\n",
       " {'label': 'NEGATIVE', 'score': 0.99828040599823},\n",
       " {'label': 'NEGATIVE', 'score': 0.9944549202919006},\n",
       " {'label': 'NEGATIVE', 'score': 0.9997929930686951}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass multiple sentences to the classifier.\n",
    "classifier(\n",
    "    [\"I love Hamburg. But it's always rainy.\",\n",
    "    \"The movie was great and the actors were bad.\",\n",
    "    \"No man ever steps in the same river twice, for it's not the same river not the same\",\n",
    "    \"I do not at all agree with the results of this useless classifier.\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What other pipelines do we have?\n",
    "\n",
    "- feature-extraction (get the vector representation of a text)\n",
    "- fill-mask\n",
    "- ner (named entity recognition)\n",
    "- question-answering\n",
    "- sentiment-analysis\n",
    "- summarization\n",
    "- text-generation\n",
    "- translation\n",
    "- zero-shot-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot classification\n",
    "\n",
    "Weâ€™ll start by tackling a more challenging task where we need to classify texts that havenâ€™t been labelled. This is a common scenario in real-world projects because annotating text is usually time-consuming and requires domain expertise. For this use case, the zero-shot-classification pipeline is very powerful: it allows you to specify which labels to use for the classification, so you donâ€™t have to rely on the labels of the pretrained model. Youâ€™ve already seen how the model can classify a sentence as positive or negative using those two labels â€” but it can also classify the text using any other set of labels you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to roberta-large-mnli (https://huggingface.co/roberta-large-mnli)\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 688/688 [00:00<00:00, 369kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.33G/1.33G [02:09<00:00, 11.0MB/s]\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at roberta-large-mnli.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 878k/878k [00:00<00:00, 1.16MB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 446k/446k [00:01<00:00, 334kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.29M/1.29M [00:02<00:00, 655kB/s]\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'This is a course about the Transformers library',\n",
       "  'labels': ['education', 'business', 'politics'],\n",
       "  'scores': [0.956234335899353, 0.026972245424985886, 0.016793372109532356]},\n",
       " {'sequence': 'The ampel coalition is planning to build the new government in december.',\n",
       "  'labels': ['politics', 'business', 'education'],\n",
       "  'scores': [0.9327414631843567, 0.04263611510396004, 0.024622347205877304]}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will return the probability for each label in sorted array (decreasing probabilty)\n",
    "\n",
    "classifier(\n",
    "    [\"This is a course about the Transformers library\",\n",
    "    \"The ampel coalition is planning to build the new government in december.\"],\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generation\n",
    "\n",
    "Now letâ€™s see how to use a pipeline to generate some text. The main idea here is that you provide a prompt and the model will auto-complete it by generating the remaining text. This is similar to the predictive text feature that is found on many phones. Text generation involves randomness, so itâ€™s normal if you donâ€™t get the same results as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 (https://huggingface.co/gpt2)\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 665/665 [00:00<00:00, 267kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 475M/475M [00:46<00:00, 10.7MB/s]\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 0.99M/0.99M [00:01<00:00, 1.01MB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 446k/446k [00:00<00:00, 679kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.29M/1.29M [00:02<00:00, 465kB/s]\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'As a Data Scientist at Adobe I mostly work with data security and privacy issues, such as security on computers.\\n\\nI do not currently see the need to hire a specialist software Engineer to support Adobe. But a good fit is my desire to provide'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"As a Data Scientist at Adobe I mostly work with\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"As a Data Engineer at Adobe I've seen many interesting things that happened, but they were completely unrelated to what I was doing for a particular job at Adobe. In fact, as a Data Engineer, it's always better to be more confident and to know more about what your customers need and are looking for.\\n\\nWe're going forward with this challenge with increasing visibility through our new feature, the 'Discovery Driven Design' and the role you should have in taking all of this information with\"},\n",
       " {'generated_text': \"As a Data Engineer at Adobe I've developed a couple of very basic tools designed to help people write fast and responsive code. I'm working on this post now since I've been able to write my own documentation. I want to thank you all for your questions on this.\\n\\nFirst step\\n\\nYou'll work on some code that creates a JSON file that you use to represent the data in your application. So, you'll open up various apps such as Office 365, Outlook and Calendar for\"},\n",
       " {'generated_text': 'As a Data Engineer at Adobe I know, it is a very real possibility that there will be an influx of new, complex features of an application by some of our students. This need to be addressed. With that being said, the same process might be possible in other areas, such as Sales and Marketing.\\n\\nThe Data Engineer in Adobe would be a great choice. Some might think they already have the necessary skills to do these types of job so it is the same. However, the problem'},\n",
       " {'generated_text': \"As a Data Engineer at Adobe I find it more productive to work on new projects and solutions. When I hear good work I think of how I could contribute on one of my own projects. Or perhaps on something I need to work on in a specific area.\\n\\nThe problem arises when the work you're doing is too difficult, and you don't know how to solve it in practice. In other words, you need to find creative ways to solve problems. However, some data engineers who have\"},\n",
       " {'generated_text': \"As a Data Engineer at Adobe I had to make a ton of choices on how we could do data visualization and I think we made a lot worth it.\\n\\nOn average we got about 3 to 4 times what we get at real time. In our job we're able to create a huge amount of data every day, which should be fairly straightforward to do manually. Unfortunately there are different tools available for different software types. The most common one we use is R, or Python. It's the\"}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"As a Data Engineer at Adobe I\", num_return_sequences=5, max_length=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a specific model from the Huggingface Hub in a pipeline: GPT-2\n",
    "\n",
    "Go to the Model Hub and click on the corresponding tag on the left to display only the supported models for that task. You should get to a page like this one.\n",
    "\n",
    "Letâ€™s try the distilgpt2 model! Hereâ€™s how to load it in the same pipeline as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import pipeline, set_seed\n",
    "# generator = pipeline('text-generation', model='distilgpt2')\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "# set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hello, I\\'m a language model, but I\\'m not sure what the \"classification criteria\" are. My classifying is actually the standard definition'},\n",
       " {'generated_text': \"Hello, I'm a language model, so I don't go through the complicated things in programming to create a nice language. So a lot of things\"},\n",
       " {'generated_text': \"Hello, I'm a language model, the language is not based on anything you have seen before. In terms of how you make those code, people\"},\n",
       " {'generated_text': 'Hello, I\\'m a language model, I\\'m a person,\" he says.\\n\\nOn Thursday night, at noon on the 11th of September'},\n",
       " {'generated_text': \"Hello, I'm a language model, I'm not saying if I'll learn to code well, but if I do, then I'm done.\"}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
