{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with huggingface transformers in tensorflow.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with pipelines\n",
    "The most basic object in the ðŸ¤— Transformers library is the pipeline. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_39']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Pipeline covers preprocessing->model->post-processing\n",
    "classifier = pipeline(\"sentiment-analysis\")  # Download and cache classifier object. Default is english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.990067720413208},\n",
       " {'label': 'NEGATIVE', 'score': 0.99828040599823},\n",
       " {'label': 'NEGATIVE', 'score': 0.9944549202919006},\n",
       " {'label': 'NEGATIVE', 'score': 0.9997929930686951}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass multiple sentences to the classifier.\n",
    "classifier(\n",
    "    [\"I love Hamburg. But it's always rainy.\",\n",
    "    \"The movie was great and the actors were bad.\",\n",
    "    \"No man ever steps in the same river twice, for it's not the same river not the same\",\n",
    "    \"I do not at all agree with the results of this useless classifier.\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What other pipelines do we have?\n",
    "\n",
    "- feature-extraction (get the vector representation of a text)\n",
    "- fill-mask\n",
    "- ner (named entity recognition)\n",
    "- question-answering\n",
    "- sentiment-analysis\n",
    "- summarization\n",
    "- text-generation\n",
    "- translation\n",
    "- zero-shot-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot classification\n",
    "\n",
    "Weâ€™ll start by tackling a more challenging task where we need to classify texts that havenâ€™t been labelled. This is a common scenario in real-world projects because annotating text is usually time-consuming and requires domain expertise. For this use case, the zero-shot-classification pipeline is very powerful: it allows you to specify which labels to use for the classification, so you donâ€™t have to rely on the labels of the pretrained model. Youâ€™ve already seen how the model can classify a sentence as positive or negative using those two labels â€” but it can also classify the text using any other set of labels you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to roberta-large-mnli (https://huggingface.co/roberta-large-mnli)\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 688/688 [00:00<00:00, 369kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.33G/1.33G [02:09<00:00, 11.0MB/s]\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at roberta-large-mnli.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 878k/878k [00:00<00:00, 1.16MB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 446k/446k [00:01<00:00, 334kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.29M/1.29M [00:02<00:00, 655kB/s]\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'This is a course about the Transformers library',\n",
       "  'labels': ['education', 'business', 'politics'],\n",
       "  'scores': [0.956234335899353, 0.026972245424985886, 0.016793372109532356]},\n",
       " {'sequence': 'The ampel coalition is planning to build the new government in december.',\n",
       "  'labels': ['politics', 'business', 'education'],\n",
       "  'scores': [0.9327414631843567, 0.04263611510396004, 0.024622347205877304]}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will return the probability for each label in sorted array (decreasing probabilty)\n",
    "\n",
    "classifier(\n",
    "    [\"This is a course about the Transformers library\",\n",
    "    \"The ampel coalition is planning to build the new government in december.\"],\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generation\n",
    "\n",
    "Now letâ€™s see how to use a pipeline to generate some text. The main idea here is that you provide a prompt and the model will auto-complete it by generating the remaining text. This is similar to the predictive text feature that is found on many phones. Text generation involves randomness, so itâ€™s normal if you donâ€™t get the same results as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 (https://huggingface.co/gpt2)\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 665/665 [00:00<00:00, 267kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 475M/475M [00:46<00:00, 10.7MB/s]\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 0.99M/0.99M [00:01<00:00, 1.01MB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 446k/446k [00:00<00:00, 679kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.29M/1.29M [00:02<00:00, 465kB/s]\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'As a Data Scientist at Adobe I mostly work with data security and privacy issues, such as security on computers.\\n\\nI do not currently see the need to hire a specialist software Engineer to support Adobe. But a good fit is my desire to provide'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"As a Data Scientist at Adobe I mostly work with\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"As a Data Engineer at Adobe I've seen many interesting things that happened, but they were completely unrelated to what I was doing for a particular job at Adobe. In fact, as a Data Engineer, it's always better to be more confident and to know more about what your customers need and are looking for.\\n\\nWe're going forward with this challenge with increasing visibility through our new feature, the 'Discovery Driven Design' and the role you should have in taking all of this information with\"},\n",
       " {'generated_text': \"As a Data Engineer at Adobe I've developed a couple of very basic tools designed to help people write fast and responsive code. I'm working on this post now since I've been able to write my own documentation. I want to thank you all for your questions on this.\\n\\nFirst step\\n\\nYou'll work on some code that creates a JSON file that you use to represent the data in your application. So, you'll open up various apps such as Office 365, Outlook and Calendar for\"},\n",
       " {'generated_text': 'As a Data Engineer at Adobe I know, it is a very real possibility that there will be an influx of new, complex features of an application by some of our students. This need to be addressed. With that being said, the same process might be possible in other areas, such as Sales and Marketing.\\n\\nThe Data Engineer in Adobe would be a great choice. Some might think they already have the necessary skills to do these types of job so it is the same. However, the problem'},\n",
       " {'generated_text': \"As a Data Engineer at Adobe I find it more productive to work on new projects and solutions. When I hear good work I think of how I could contribute on one of my own projects. Or perhaps on something I need to work on in a specific area.\\n\\nThe problem arises when the work you're doing is too difficult, and you don't know how to solve it in practice. In other words, you need to find creative ways to solve problems. However, some data engineers who have\"},\n",
       " {'generated_text': \"As a Data Engineer at Adobe I had to make a ton of choices on how we could do data visualization and I think we made a lot worth it.\\n\\nOn average we got about 3 to 4 times what we get at real time. In our job we're able to create a huge amount of data every day, which should be fairly straightforward to do manually. Unfortunately there are different tools available for different software types. The most common one we use is R, or Python. It's the\"}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"As a Data Engineer at Adobe I\", num_return_sequences=5, max_length=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a specific model from the Huggingface Hub in a pipeline: GPT-2\n",
    "\n",
    "Go to the Model Hub and click on the corresponding tag on the left to display only the supported models for that task. You should get to a page like this one.\n",
    "\n",
    "Letâ€™s try the distilgpt2 model! Hereâ€™s how to load it in the same pipeline as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import pipeline, set_seed\n",
    "# generator = pipeline('text-generation', model='distilgpt2')\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "# set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hello, I\\'m a language model, but I\\'m not sure what the \"classification criteria\" are. My classifying is actually the standard definition'},\n",
       " {'generated_text': \"Hello, I'm a language model, so I don't go through the complicated things in programming to create a nice language. So a lot of things\"},\n",
       " {'generated_text': \"Hello, I'm a language model, the language is not based on anything you have seen before. In terms of how you make those code, people\"},\n",
       " {'generated_text': 'Hello, I\\'m a language model, I\\'m a person,\" he says.\\n\\nOn Thursday night, at noon on the 11th of September'},\n",
       " {'generated_text': \"Hello, I'm a language model, I'm not saying if I'll learn to code well, but if I do, then I'm done.\"}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations and bias\n",
    "\n",
    "The training data used for this model has not been released as a dataset one can browse. We know it contains a lot of unfiltered content from the internet, which is far from neutral. As the openAI team themselves point out in their model card:\n",
    "\n",
    "_Because large-scale language models like GPT-2 do not distinguish fact from fiction, we donâ€™t support use-cases that require the generated text to be true._\n",
    "\n",
    "_Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do not recommend that they be deployed into systems that interact with humans > unless the deployers first carry out a study of biases relevant to the intended use-case. We found no statistically significant difference in gender, race, and religious bias probes between 774M and 1.5B, implying all versions of GPT-2 should be approached with similar levels of caution around use cases that are sensitive to biases around human attributes._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The White man worked as a sales assistant at another'},\n",
       " {'generated_text': 'The White man worked as a security guard in the'},\n",
       " {'generated_text': 'The White man worked as a bartender at a bank'},\n",
       " {'generated_text': 'The White man worked as a car salesman in Richmond'},\n",
       " {'generated_text': 'The White man worked as a carpenter by day'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"The White man worked as a\", max_length=10, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The Black man worked as a private investigator after graduating'},\n",
       " {'generated_text': 'The Black man worked as a security guard; the'},\n",
       " {'generated_text': 'The Black man worked as a bartender at a bank'},\n",
       " {'generated_text': 'The Black man worked as a car salesman in Richmond'},\n",
       " {'generated_text': 'The Black man worked as a carpenter or metal'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "generator(\"The Black man worked as a\", max_length=10, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 865/865 [00:00<00:00, 388kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 475M/475M [01:11<00:00, 6.95MB/s]\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at dbmdz/german-gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.37M/1.37M [00:02<00:00, 709kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der Sinn des Lebens ist es ja, das Leben und diese Erfahrung miteinander zu verbinden oder beides.\n",
      "Eine neue Dimension wird in diesem Prozess sichtbar, der \"Neue Weg\".\n",
      "Das alte Paradigma ist das, das wir das \"Wissen, was ein GefÃ¼hl von Liebe\" nennen.\n",
      "Wenn jemand ein Liebesleben oder ein anderes LebensgefÃ¼hl liebt, dann ist es das, das man hat.\n",
      "Alles, was er fÃ¼hlt und fÃ¼hlt, ist das, was er fÃ¼hlt.\n",
      "Es gibt in diesem Augenblick\n"
     ]
    }
   ],
   "source": [
    "# Testing another language\n",
    "pipe = pipeline('text-generation', model=\"dbmdz/german-gpt2\",\n",
    "                 tokenizer=\"dbmdz/german-gpt2\")\n",
    "\n",
    "text = pipe(\"Der Sinn des Lebens ist es\", max_length=100)[0][\"generated_text\"]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base (https://huggingface.co/distilroberta-base)\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 480/480 [00:00<00:00, 217kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 465M/465M [01:16<00:00, 6.34MB/s]\n",
      "All model checkpoint layers were used when initializing TFRobertaForMaskedLM.\n",
      "\n",
      "All the layers of TFRobertaForMaskedLM were initialized from the model checkpoint at distilroberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForMaskedLM for predictions without further training.\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 878k/878k [00:01<00:00, 689kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 446k/446k [00:00<00:00, 700kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.29M/1.29M [00:01<00:00, 714kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'This course will teach you all about mathematical models.',\n",
       "  'score': 0.19619612395763397,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical'},\n",
       " {'sequence': 'This course will teach you all about computational models.',\n",
       "  'score': 0.040526993572711945,\n",
       "  'token': 38163,\n",
       "  'token_str': ' computational'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\")\n",
    "unmasker(\"This course will teach you all about <mask> models.\", top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top_k argument controls how many possibilities you want to be displayed. Note that here the model fills in the special <mask> word, which is often referred to as a mask token. Other mask-filling models might have different mask tokens, so itâ€™s always good to verify the proper mask word when exploring other models. One way to check it is by looking at the mask word used in the widget."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
